{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5dcb598-1aab-479b-8efd-135b3a2f3250",
   "metadata": {},
   "source": [
    "## Training basic linear models with integrated feature + segmentation for census prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519948b-0cb5-4b50-83e5-6681d48a1088",
   "metadata": {},
   "source": [
    "### 1. PCA prediction on quantile ranges of sociodemographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28503b54-88bc-4d13-9d53-35c17d02c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -------------------------\n",
    "# 1️⃣ Load PCA-Transformed Features\n",
    "# -------------------------\n",
    "folder_path = \"/Users/etoileboots/Desktop/CAPSTONE/res_net/features_and_visualizations\"  # Update if necessary\n",
    "feature_files = [f for f in os.listdir(folder_path) if f.endswith(\".npy\")]\n",
    "\n",
    "tract_features = {}\n",
    "\n",
    "for file in feature_files:\n",
    "    tract_name = file.replace(\".npy\", \"\")  # Extract tract identifier\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    features_dict = np.load(file_path, allow_pickle=True).item()\n",
    "    tract_features[tract_name] = np.array(list(features_dict.values()))  # Convert to NumPy array\n",
    "\n",
    "# Combine all feature vectors for PCA\n",
    "all_features = np.vstack(list(tract_features.values()))\n",
    "\n",
    "# Apply PCA - Keep enough components to explain 95% variance\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(all_features)\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ Transform and Store Tract-Level PCA Features\n",
    "# -------------------------\n",
    "tract_pca_results = {}\n",
    "\n",
    "for tract in tract_features:\n",
    "    transformed_features = pca.transform(tract_features[tract])\n",
    "    tract_pca_results[tract] = transformed_features.mean(axis=0)  # Use mean PCA features per tract\n",
    "\n",
    "# Convert to DataFrame\n",
    "pca_df = pd.DataFrame.from_dict(tract_pca_results, orient=\"index\")\n",
    "\n",
    "# Ensure correct column naming\n",
    "pca_df.columns = [f\"PCA_{i+1}\" for i in range(pca_df.shape[1])]\n",
    "\n",
    "# Convert index to a proper column for merging\n",
    "pca_df.index.name = \"GEOID20\"\n",
    "pca_df.reset_index(inplace=True)\n",
    "\n",
    "# Print Results\n",
    "print(\"PCA-transformed feature DataFrame shape:\", pca_df.shape)\n",
    "print(pca_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f150c1-e2aa-4614-8cd6-f589802cd67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the geiod data frame \n",
    "pca_df.index = pca_df.index.astype(str)  # Ensure index is string type\n",
    "pca_df[\"GEOID20\"] = pca_df[\"GEOID20\"].astype(str).str.replace(\"features_\", \"\", regex=False)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d0e7a-2b42-463f-85d1-1ba79d3c5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOCIODEMOGRAPHICS\n",
    "# -------------------------\n",
    "# 2️⃣ Load Sociodemographic Data\n",
    "# -------------------------\n",
    "csv_path = \"/Volumes/MRDALLMAYR/data/census_data/census_data_harris_county_2022.csv\"  # Update if necessary\n",
    "socio_df = pd.read_csv(csv_path, dtype={\"GEOID20\": str})  # Load as string to avoid type mismatches\n",
    "\n",
    "# Select relevant columns (remove categorical or ID columns)\n",
    "target_columns = [\n",
    "    \"median_income\", \"pct_income_below_poverty\", \"pct_income_above_poverty\", \n",
    "    \"pct_no_vehicle\", \"pct_public_transport\", \"pct_commute_more_than_60\"\n",
    "]\n",
    "\n",
    "socio_df = socio_df[[\"GEOID20\"] + target_columns]  # Keep only relevant data\n",
    "socio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b1625-6ec0-4033-a967-b15674961d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 3️⃣ Merge PCA Features and Sociodemographic Data\n",
    "# -------------------------\n",
    "\n",
    "socio_df[\"GEOID20\"] = socio_df[\"GEOID20\"].astype(str)\n",
    "pca_df[\"GEOID20\"] = pca_df[\"GEOID20\"].astype(str)\n",
    "socio_PCA_df = socio_df.merge(pca_df, on=\"GEOID20\")\n",
    "socio_PCA_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f92a25-792d-4a6e-af96-a95b6cbfbe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix for all numeric columns\n",
    "corr_matrix = socio_PCA_df.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "# Extract correlations between PCA features and sociodemographic variables\n",
    "pca_feature_cols = [col for col in socio_PCA_df.columns if col.startswith(\"PCA\")]\n",
    "socio_cols = [\"median_income\", \"pct_income_below_poverty\", \"pct_income_above_poverty\",\n",
    "              \"pct_no_vehicle\", \"pct_public_transport\", \"pct_commute_more_than_60\"]  # Adjust based on dataset\n",
    "\n",
    "# Subset correlation matrix to show only PCA vs. Sociodemographic correlations\n",
    "correlation_results = corr_matrix.loc[pca_feature_cols, socio_cols]\n",
    "\n",
    "# Sort PCA components by their highest absolute correlation with any target variable\n",
    "correlation_rank = correlation_results.abs().max(axis=1).sort_values(ascending=False)\n",
    "\n",
    "# Print the top 10 PCA components with the highest correlation to any sociodemographic variable\n",
    "print(\"\\nTop PCA Features with Highest Correlation to Sociodemographic Factors:\")\n",
    "print(correlation_rank.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e8efd-20b3-482b-a845-31764220b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute absolute correlations and rank PCA features\n",
    "correlation_rank = correlation_results.abs().max(axis=1).sort_values(ascending=False)\n",
    "\n",
    "# Select the top 10 most correlated PCA features\n",
    "top_pca_features = correlation_rank.head(10).index\n",
    "\n",
    "# Subset correlation matrix for only the top PCA components\n",
    "top_correlation_results = correlation_results.loc[top_pca_features]\n",
    "\n",
    "# Plot heatmap for only the top PCA features\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(top_correlation_results, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Top 10 PCA Features Correlated with Sociodemographic Variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d3bfc-b962-4670-8912-b5a728628b3c",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fdf8f4-1e6c-4fb4-aeb4-b1e9fff04992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning median income into a binary classification problem \n",
    "# Check if there are any negative median income values\n",
    "negative_values = socio_PCA_df[socio_PCA_df[\"median_income\"] < 0]\n",
    "\n",
    "if not negative_values.empty:\n",
    "    print(\"Warning: Found negative median income values!\")\n",
    "    print(negative_values)\n",
    "else:\n",
    "    print(\"No negative median income values found.\")\n",
    "\n",
    "# Remove rows where median_income is negative\n",
    "socio_PCA_df = socio_PCA_df[socio_PCA_df[\"median_income\"] >= 0]\n",
    "\n",
    "# Reset index after removal\n",
    "socio_PCA_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Verify that no negative values remain\n",
    "print(\"Negative median income values remaining:\", (socio_PCA_df[\"median_income\"] < 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e804a-0eff-47d4-8116-4f9cac6fd37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_median = socio_PCA_df[\"median_income\"].median()\n",
    "print(f\"Median Income Threshold: {income_median} USD, annually\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8147e8-f8f9-49c7-b3f3-2a08496774c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a binary index variable: \n",
    "socio_PCA_df[\"median_income_binary\"] = (socio_PCA_df[\"median_income\"] >= income_median).astype(int)\n",
    "print(socio_PCA_df[\"median_income_binary\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a6513-a515-4666-9a6f-72644d934d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# -------------------------\n",
    "# 1️⃣ Prepare Data\n",
    "# -------------------------\n",
    "# Select PCA features\n",
    "pca_feature_cols = [col for col in socio_PCA_df.columns if col.startswith(\"PCA\")]\n",
    "X = socio_PCA_df[pca_feature_cols].values  # Convert to NumPy array\n",
    "y = socio_PCA_df[\"median_income_binary\"].values  # Binary labels (0 or 1)\n",
    "\n",
    "# Standardize features for SVM & Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ Train and Evaluate Models\n",
    "# -------------------------\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Support Vector Machine\": SVC(kernel=\"rbf\", probability=True),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)  # Train model\n",
    "    \n",
    "    y_pred = model.predict(X_test)  # Make predictions\n",
    "    \n",
    "    # Compute accuracy & classification report\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": report[\"1\"][\"precision\"],  # Precision for label 1 (higher income)\n",
    "        \"Recall\": report[\"1\"][\"recall\"],        # Recall for label 1\n",
    "        \"F1-Score\": report[\"1\"][\"f1-score\"]      # F1-score for label 1\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ Display Model Performance\n",
    "# -------------------------\n",
    "results_df = pd.DataFrame(results).T  # Convert to DataFrame\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(results_df)\n",
    "\n",
    "# -------------------------\n",
    "# 4️⃣ Visualize Model Performance\n",
    "# -------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "results_df.plot(kind=\"bar\", figsize=(12,6), colormap=\"coolwarm\")\n",
    "plt.title(\"Model Performance Comparison on Binary Median Income Binary\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed1fa2-aa18-48f5-a254-9680e0df028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, r2_score\n",
    "\n",
    "# -------------------------\n",
    "# 1️⃣ Define Target Variables & Prepare Data\n",
    "# -------------------------\n",
    "# Select PCA features\n",
    "pca_feature_cols = [col for col in socio_PCA_df.columns if col.startswith(\"PCA\")]\n",
    "X = socio_PCA_df[pca_feature_cols].values  # Convert PCA features to NumPy array\n",
    "\n",
    "# Define **classification** (binary) and **regression** (continuous) target variables\n",
    "binary_targets = [\"median_income\"]  # Targets that need binary classification\n",
    "regression_targets = [\n",
    "    \"pct_income_below_poverty\", \"pct_income_above_poverty\",\n",
    "    \"pct_no_vehicle\", \"pct_public_transport\", \"pct_commute_more_than_60\"\n",
    "]  # Continuous regression targets\n",
    "\n",
    "# Convert each binary target variable into 0 = below median, 1 = above median\n",
    "for target in binary_targets:\n",
    "    median_value = socio_PCA_df[target].median()\n",
    "    socio_PCA_df[f\"{target}_binary\"] = (socio_PCA_df[target] >= median_value).astype(int)\n",
    "\n",
    "# Standardize features for better model performance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ Train and Evaluate Classification Models (For Binary Targets)\n",
    "# -------------------------\n",
    "classification_models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Support Vector Machine\": SVC(kernel=\"rbf\", probability=True),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "classification_results = {}\n",
    "\n",
    "for target in binary_targets:\n",
    "    print(f\"\\n=== Training Classification Models for {target}_binary ===\")\n",
    "\n",
    "    # Define binary target variable\n",
    "    y = socio_PCA_df[f\"{target}_binary\"].values\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model in classification_models.items():\n",
    "        print(f\"\\nTraining {name} on {target}...\")\n",
    "        model.fit(X_train, y_train)  # Train model\n",
    "\n",
    "        y_pred = model.predict(X_test)  # Make predictions\n",
    "\n",
    "        # Compute accuracy & classification report\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": report[\"1\"][\"precision\"],  # Precision for label 1\n",
    "            \"Recall\": report[\"1\"][\"recall\"],        # Recall for label 1\n",
    "            \"F1-Score\": report[\"1\"][\"f1-score\"]      # F1-score for label 1\n",
    "        }\n",
    "\n",
    "    # Store classification results\n",
    "    classification_results[target] = results\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ Train and Evaluate Regression Models (For Continuous Targets)\n",
    "# -------------------------\n",
    "regression_models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost Regressor\": XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "}\n",
    "\n",
    "regression_results = {}\n",
    "\n",
    "for target in regression_targets:\n",
    "    print(f\"\\n=== Training Regression Models for {target} ===\")\n",
    "\n",
    "    # Define continuous target variable\n",
    "    y = socio_PCA_df[target].values\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model in regression_models.items():\n",
    "        print(f\"\\nTraining {name} on {target}...\")\n",
    "        model.fit(X_train, y_train)  # Train model\n",
    "\n",
    "        y_pred = model.predict(X_test)  # Make predictions\n",
    "\n",
    "        # Compute MAE and R^2 Score\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R² Score\": r2\n",
    "        }\n",
    "\n",
    "    # Store regression results\n",
    "    regression_results[target] = results\n",
    "\n",
    "# -------------------------\n",
    "# 4️⃣ Display and Visualize Model Performance\n",
    "# -------------------------\n",
    "# Classification results\n",
    "for target, results in classification_results.items():\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(f\"\\nClassification Model Performance for {target}_binary:\")\n",
    "    print(results_df)\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    results_df.plot(kind=\"bar\", figsize=(12,6), colormap=\"coolwarm\")\n",
    "    plt.title(f\"Classification Model Performance for {target}_binary\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis=\"y\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Regression results\n",
    "for target, results in regression_results.items():\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(f\"\\nRegression Model Performance for {target}:\")\n",
    "    print(results_df)\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    results_df.plot(kind=\"bar\", figsize=(12,6), colormap=\"coolwarm\")\n",
    "    plt.title(f\"Regression Model Performance for {target}\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis=\"y\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d2406-c2e4-476b-814c-e4b58561edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# -------------------------\n",
    "# 1️⃣ Prepare Data\n",
    "# -------------------------\n",
    "# Select PCA features as input\n",
    "pca_feature_cols = [col for col in socio_PCA_df.columns if col.startswith(\"PCA\")]\n",
    "X = socio_PCA_df[pca_feature_cols].values  # Convert DataFrame to NumPy array\n",
    "\n",
    "# Target variable\n",
    "y = socio_PCA_df[\"median_income_binary\"].values  # Binary labels (0 or 1)\n",
    "\n",
    "# Standardize the PCA features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Normalize features for better performance\n",
    "\n",
    "# Split into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Testing set: {X_test.shape}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ Build the Neural Network\n",
    "# -------------------------\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(256, activation=\"relu\", input_shape=(X_train.shape[1],)),  # First hidden layer\n",
    "    keras.layers.Dropout(0.3),  # Dropout to prevent overfitting\n",
    "    keras.layers.Dense(128, activation=\"relu\"),  # Second hidden layer\n",
    "    keras.layers.Dropout(0.2),  # Another Dropout layer\n",
    "    keras.layers.Dense(64, activation=\"relu\"),  # Third hidden layer\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")  # Output layer (binary classification)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ Train the Model\n",
    "# -------------------------\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# -------------------------\n",
    "# 4️⃣ Evaluate the Model\n",
    "# -------------------------\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "# Compute accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# -------------------------\n",
    "# 5️⃣ Plot Training Progress\n",
    "# -------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['accuracy'], label=\"Train Accuracy\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['loss'], label=\"Train Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Model Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83bba4-5d73-4773-a525-33d58e20474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate features (X) and target variables (y)\n",
    "X = df.drop(columns=[\"GEOID20\"] + target_columns)  # Keep only PCA features\n",
    "y = df[target_columns]  # Targets for regression\n",
    "\n",
    "# -------------------------\n",
    "# 4️⃣ Train-Test Split\n",
    "# -------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -------------------------\n",
    "# 5️⃣ Train Regression Model\n",
    "# -------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize features (X)\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Standardize targets (y)\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)  # Fit & transform training targets\n",
    "y_test_scaled = scaler_y.transform(y_test)  # Transform test targets\n",
    "\n",
    "# Train model on scaled data\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict and inverse transform\n",
    "y_pred_scaled = lr.predict(X_test_scaled)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)  # Convert back to original scale\n",
    "\n",
    "# Compute separate MSE for each target\n",
    "mse_values = {}\n",
    "for i, target in enumerate(y.columns):\n",
    "    mse = mean_squared_error(y_test[target], y_pred[:, i])\n",
    "    mse_values[target] = mse\n",
    "    print(f\"MSE for {target}: {mse:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6️⃣ Train a Random Forest Model (for comparison)\n",
    "# -------------------------\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf.predict(X_test_scaled)\n",
    "\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest - MAE: {mae_rf:.4f}, R² Score: {r2_rf:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 7️⃣ Plot Feature Importance from PCA Components\n",
    "# -------------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, X_train.shape[1] + 1), np.abs(lr.coef_).mean(axis=0))  # Average importance across targets\n",
    "plt.xlabel(\"PCA Component Number\")\n",
    "plt.ylabel(\"Feature Importance (Absolute Coefficients)\")\n",
    "plt.title(\"Feature Importance of PCA Components in Regression Model\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c43ea-ae3f-4a4a-b212-e8efd97b72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define MLP Model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(y_train.shape[1])  # Output layer (same as number of target variables)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train_scaled, epochs=50, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84530cf4-78ed-4670-bbd3-9a4a82f19f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot residuals for each target variable\n",
    "for i, target in enumerate(y.columns):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(y_test[target] - y_pred[:, i], bins=30, kde=True)\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Residual Distribution for {target}\")\n",
    "    plt.axvline(0, color='r', linestyle='dashed')  # Ideal residuals should be centered at 0\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a4ae8-78b8-456c-b433-d0d1070e1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training vs. validation loss over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(model.history.history['loss'], label='Training Loss')\n",
    "plt.plot(model.history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
